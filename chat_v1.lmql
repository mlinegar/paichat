# changelog
# active development:
# - check if having multiple accounts speeds up generation
#   - how to do this? set API env file, choose from one of multiple API keys
# - add checks at beginning of each section? Is this section necessary based on previous user answer?
# - think about how to minimize total number of calls since so much delay
# - incorporate "deep canvassing" techniques into prompts
# - prevent getting stuck
# - switch from asking questions individually to giving the LLM more flexibility, not less

import lmql
#  from lmql.lib.chat import message
import json
import asyncio
import nest_asyncio
nest_asyncio.apply()
import logging
import pandas as pd
import datetime
import logging
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
)  
from pathlib import Path
#  from tinydb import TinyDB, Query
#  from BetterJSONStorage import BetterJSONStorage
from tinydb import TinyDB
from tinydb.storages import JSONStorage
from tinydb.middlewares import CachingMiddleware
import uuid
from chat.output import *
message = MessageDecorator()
#  from utils import *
# FIXME: allow option to accept user_id as input/read from url/file
#  user_id = "U12345678"
user_id = str(uuid.uuid4())
path = Path(f"output/{user_id}.db")
# set up database for storage
#  db = TinyDB(path, access_mode="r+", storage=BetterJSONStorage)
db = TinyDB(path, storage=CachingMiddleware(JSONStorage))
#  table = db.table(user_id)

### SET OPTIONS ###
small_model_name = 'openai/gpt-3.5-turbo'
large_model_name = 'openai/gpt-4'
small_model = lmql.model(small_model_name)
large_model = lmql.model(large_model_name)
#  lmql.set_default_model('chatgpt')
#  lmql.set_default_model('gpt-4')
max_depth = 3
script_name = "immigration_script"


def append_to_database(db, data):
    db.insert(data)
    db.storage.flush() # force save

with open(f'data/{script_name}.json') as f:
    full_script = json.load(f)
    script_details = full_script[0]
    script_description = script_details['script_description']
    script_length = len(full_script)
    script = full_script[1:script_length]

# add user info, model info, etc.
append_to_database(db, {
    "user_id": user_id,
    "script_name": script_name,
    "script_description": script_description,
    "large_model": large_model_name,
    "small_model": small_model_name
})

system_prompt = '''
You are a trained volunteer canvasser named Brook, an expert in "deep canvassing" techniques. Your goal is to have a non-judgmental, empathetic conversation with the user about {script_description} to understand their views and experiences. 

Build rapport and trust through active listening. Ask open-ended questions to elicit personal stories related to the topic. Acknowledge the user's experiences without arguing or telling them they are wrong. Aim to find common ground in shared values and struggles.

Structure the conversation as follows:
1. Create a non-judgmental context by asking neutral questions about their views. Express genuine interest in understanding their perspective. 
2. Ask questions to uncover personal experiences underpinning their views. Share a brief relevant story of your own to build connection.
3. Ask questions that encourage the user to consider the experiences of those impacted by the issue, to build empathy. 
4. Explain why the conversation is deeply important to you personally.
5. Gently explore any contradictions between the user's stated views and personal experiences. Ask what is on their mind after this discussion.
6. Briefly address any specific concerns with objective facts, but avoid arguing.
7. Encourage the user to reflect on if and why their views may have shifted.

Keep language simple and concise, broken into short paragraphs, as the user may be on a phone. Accept any relevant response, even if potentially offensive, to maintain a non-judgmental tone. Focus on drawing out the user's stories and perspectives, not on asking scripted questions. The conversation should feel natural, with the ultimate aim of encouraging the user to actively process new perspectives.
'''

def setup_logger(name, log_file, level=logging.INFO):
    """Function to setup as many loggers as you want"""
    handler = logging.FileHandler(log_file, mode='w')        
    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)

    return logger

chat_logger = setup_logger('chat_logger', 'logging/' + user_id + '.log')


def log_messages(chat_logger, messages, level="info"):
    """
    Logs messages with specified logging level in a structured manner.

    Args:
    - chat_logger: The logger object.
    - messages: A dictionary where keys are message labels and values are the messages themselves.
    - level: The logging level as a string (e.g., "info", "error").
    """
    for label, value in messages.items():
        formatted_message = f"{label}: {value}"
        if level == "info" : 
            chat_logger.info(formatted_message)
        elif level == "error" : 
            chat_logger.error(formatted_message)
        # Add more logging levels here as needed.

@lmql.query(chunksize=16)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def yes_no():
    '''lmql
    "Please respond with just 'Yes' or 'No', whichever you think is the best answer."
    "A:[FLAG]" #where STOPS_AT(FLAG, '.') # FLAG in ['Yes.', 'No.'] and 
    return "yes" in FLAG.lower()
    '''

@lmql.query
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def chain_of_thought():
    '''lmql
    "Let's break this down step-by-step to determine the best way to proceed."
    "Reasoning:[THOUGHTS]"
    return THOUGHTS
    '''
        
@lmql.query
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def print_text(text):
    '''lmql
    "{text}"
    '''

@lmql.query
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def include(text):
    '''lmql
    "Rephrase the following question to be more conversational and empathetic, while preserving its core meaning: {text}"
    "[RESPONSE]"
    return RESPONSE
    '''

@lmql.query(model=large_model)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def ask_question(question_text, last_bot_response, user_response, verbatim, recalled_context="", max_length=256):
    '''lmql
    "Remember, my last message to the user was: {last_bot_response}"
    if not user_response=="":
        "The user then shared: {user_response}."
    if not recalled_context=="":
        "Some additional context about the user: {recalled_context}."
    "My response should build on what the user said and feel like a natural continuation of our dialogue."
    "If the user asked me anything, I should thoughtfully address their questions or points."
    if verbatim:
        "[QUESTION: print_text(question_text)]"
    else:
        "Here is the question I'm trying to communicate to the user, rephrased to be more conversational:{question_text}[QUESTION: include(QUESTION)]" where len(TOKENS(QUESTION)) < max_length
    return QUESTION
    '''

@lmql.query(model=small_model)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def did_user_answer(question_text, answer_text, prev_answer_text):
    '''lmql
    "The question I asked was: {question_text}"
    "The user's latest response was: {answer_text}"
    "Their previous responses on this topic were: {prev_answer_text}"
    "Keeping in mind the user may be typing on a phone and give short or incomplete answers:"
    "Did the user's response directly address the question asked?" 
    "[COT: chain_of_thought]"
    "[ANSWERED: yes_no]"
    return ANSWERED
    '''

@lmql.query(model=small_model)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def get_user_answer(question_text, answer_text, prev_answer_text):
    '''lmql
    "The question I asked was: {question_text}"
    "The user's latest response was: {answer_text}"
    "Their previous responses on this were: {prev_answer_text}" 
    "What key points did the user communicate as an answer to the question? If they didn't directly answer, summarize 'None'."
    "[COT: chain_of_thought]"
    "User's main points:[ANSWER]"
    return ANSWER
    '''

@lmql.query(model=small_model)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def assistant_response_needed(question_text, answer_text, prev_answer_text):
    '''lmql
    "The question I asked was: {question_text}"
    "The user's latest response was: {answer_text}"
    "To maintain a natural dialogue, does it make sense for me to respond to the user before asking the next question?"
    "I should respond if the user's answer was unclear, incomplete, or raised new points for discussion." 
    "But if their response was sufficient, I can move on."
    "[COT: chain_of_thought]"
    "[ASSISTANT_RESPONSE_NEEDED: yes_no]"
    return ASSISTANT_RESPONSE_NEEDED
    '''

@lmql.query(model=small_model)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def gen_flag(flag_text):
    '''lmql
    "Based on our discussion: {flag_text}"
    "[COT: chain_of_thought]" 
    "[ANS: yes_no]"
    return ANS
    '''

@lmql.query(model=large_model)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def question_followup(question, user_input, errors, tries, max_depth):
    '''lmql
    "I asked the user: {question}"
    "Their response was: {user_input}"
    "However, this doesn't fully answer the question."
    "How can I empathetically acknowledge what they shared while gently guiding our discussion back to the original question?"
    "I want to ensure I understand their perspective."
    if errors >= max_depth or tries >= max_depth:
        "We've explored this question from a few angles now. To be respectful of the user's time, I'll move us along to another topic." 
        "What's a natural way to transition to the next part of our conversation?"
    "[FOLLOW_UP]"
    return FOLLOW_UP
    '''

@retry(wait=wait_random_exponential(min=1, max=30), stop=stop_after_attempt(6))
async def execute_response_analysis(question_text, answer_text, prev_answer_text):
    user_answered_task = asyncio.create_task(did_user_answer(question_text, answer_text, prev_answer_text))
    user_answer_task = asyncio.create_task(get_user_answer(question_text, answer_text, prev_answer_text))
    assistant_response_needed_task = asyncio.create_task(assistant_response_needed(question_text, answer_text, prev_answer_text))

    user_answered_ANS = await user_answered_task

    if not user_answered_ANS:
        return False, "", True

    # Await other tasks only if necessary
    user_answer_ANS = await user_answer_task
    assistant_response_needed_ANS = await assistant_response_needed_task

    return True, user_answer_ANS, assistant_response_needed_ANS

@lmql.query(model=small_model)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def is_section_relevant(conversation_context, section_questions):
    '''lmql
    "Given the conversation context: {conversation_context}"
    "And the following questions for this section: {section_questions}"
    "Analyze the set of questions to determine if this section is still relevant to ask the user, or if it has already been sufficiently answered."
    "[COT: chain_of_thought]"
    "[SECTION_RELEVANT: yes_no]"
    return SECTION_RELEVANT
    '''
#  minimal working example (for debugging)
#  argmax(chunksize=128)
#  "{:system} {system_prompt}"
#  #  "{:assistant} {intro_text} [@message INTRO]"
#  while True:
#      user_input = await input()
#      "{:user} {user_input}"
#      "{:assistant} External Answer: [@message ANSWER]"

@retry(wait=wait_random_exponential(min=1, max=30), stop=stop_after_attempt(6))
async def execute_response_analysis(question_text, answer_text, prev_answer_text, context, step_goal, step_text):
    user_answered_task = asyncio.create_task(did_user_answer(question_text, answer_text, prev_answer_text, context, step_goal, step_text))
    user_answer_task = asyncio.create_task(get_user_answer(question_text, answer_text, prev_answer_text, context, step_goal, step_text))
    assistant_response_needed_task = asyncio.create_task(assistant_response_needed(question_text, answer_text, prev_answer_text, context, step_goal, step_text))

    user_answered_ANS = await user_answered_task

    if not user_answered_ANS:
        return False, "", True

    # Await other tasks only if necessary
    user_answer_ANS = await user_answer_task
    assistant_response_needed_ANS = await assistant_response_needed_task

    return True, user_answer_ANS, assistant_response_needed_ANS

argmax(chunksize=128)
"{:system} {system_prompt}"
"{:assistant} Introduce yourself and the purpose of the conversation. Emphasize your role as an active listener here to understand the user's perspective. [@message INTRO]"
last_bot_response = INTRO

conversation_context = ""
conversation_id = str(uuid.uuid4())  # Generate a unique conversation ID

for i, step in enumerate(script):
    
    step_num = step['step_num']
    step_check = f"Step Number: {step_num}"
    chat_logger.info(step_check)
    step_goal = step['step_goal']
    step_text = step['step_text']
    questions = step['questions']
    requires_answer = []
    step_questions = ""
    for qq, _ in enumerate(questions):
        requires_answer.append(questions[qq]['requires_user_answer'])
        step_questions += step_questions + questions[qq]['question_text'] + "\n"
    any_answers_required = any(requires_answer)
    
    if any_answers_required:  
        section_relevant = True      
    else:
        section_relevant = await is_section_relevant(conversation_context, step_questions)
    
    if not section_relevant:
        print("Skipping section")
        continue

    step_flag = None
    user_response = ""
    step_responses = ""

    for qq, question in enumerate(questions):
        tries = 0
        errors = 0
        
        question_text = questions[qq]['question_text']
        if qq < len(questions) - 1:
            next_question_text = questions[qq+1]['question_text']
        else:
            next_question_text = "End of step: " + step_text

        requires_user_answer = question['requires_user_answer']
        allow_assistant_response = question['allow_assistant_response']
        flag_required = question['flag_required']
        generate_flag = question['generate_flag']
        verbatim = question['verbatim']
        recallable = question['recallable']
        max_length = question['max_length']
        use_tool = question['use_tool']
        tool_name = question['tool_name']
        log_messages(chat_logger, {"Step Number" : step_num, "Step Text" : step_text, "Question Number" : qq, "Question Text" : question_text, "Requires User Answer" : requires_user_answer, "Allow Assistant Response" : allow_assistant_response, "Flag Required" : flag_required, "Step Flag" : step_flag, "Generate Flag" : generate_flag, "Recallable" : recallable, "Verbatim" : verbatim})
        
        if (not flag_required is None) and (not step_flag == flag_required):
            continue

        QUESTION = await ask_question(question_text, last_bot_response, user_response, verbatim, recalled_context=conversation_context, max_length=max_length, tool_name = tool_name)    
        "{:assistant} [@message QUESTION_PRINT: print_text(QUESTION)]"

        last_bot_response = QUESTION
        log_messages(chat_logger, {"Question" :  QUESTION})

        append_to_database(db,{
            "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "conversation_id": conversation_id,
            "step_num": step_num,
            "step_text": step_text,
            "question_num": qq,
            "question_text": question_text,
            "bot_output": QUESTION,
            "type": "assistant_message"
        })

        continue_conversation = requires_user_answer
        while continue_conversation and tries <= max_depth and errors <= max_depth:
            
            user_input = await input()
            step_responses = step_responses + "\n" + user_input
            "{:user} {user_input}"    

            log_messages(chat_logger, {"User Input": user_input}, level="info")

            append_to_database(db,{
                "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "conversation_id": conversation_id,
                "step_num": step_num,
                "step_text": step_text,
                "question_num": qq,
                "tries": tries,
                "question_text": question_text,
                "recallable": recallable,
                "user_input": user_input,
                "type": "user_message"
            })

            if user_input == "SKIP":
                continue_conversation = False
                QUESTION_ANSWERED = False
                QUESTION_ANSWER = "SKIP"
                continue

            QUESTION_ANSWERED, QUESTION_ANSWER, ASSISTANT_RESPONSE_NEEDED = await execute_response_analysis(question_text, user_input, step_responses, conversation_context, step_goal, step_text)

            log_messages(chat_logger,
                            {"QUESTION_ANSWERED" : QUESTION_ANSWERED,
                            "QUESTION_ANSWER" : QUESTION_ANSWER,
                            "ASSISTANT_RESPONSE_NEEDED" : ASSISTANT_RESPONSE_NEEDED})

            append_to_database(db,{
                "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "conversation_id": conversation_id,
                "step_num": step_num,
                "step_text": step_text,  
                "question_num": qq,
                "tries": tries,
                "question_text": question_text,
                "recallable": recallable,
                "bot_output": QUESTION,
                "user_input": user_input,
                "answer": QUESTION_ANSWER,
                "answered": QUESTION_ANSWERED,
                "assistant_response_needed": ASSISTANT_RESPONSE_NEEDED
            })

            if QUESTION_ANSWERED:
                if recallable:
                    conversation_context = conversation_context + "\nQ:" + question_text + "\nA:" + QUESTION_ANSWER

                if generate_flag:
                    flag_text = question['flag_text']
                    step_flag = await gen_flag(flag_text)
                    log_messages(chat_logger, {"Flag question" :  flag_text, "Step flag" :  step_flag})
                    append_to_database(db,{
                        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "conversation_id": conversation_id,
                        "step_num": step_num,
                        "step_text": step_text,
                        "question_num": qq,
                        "tries": tries,
                        "flag_text": flag_text,
                        "step_flag": step_flag,
                        "type": "flag"
                    })
                
                if ASSISTANT_RESPONSE_NEEDED:
                    tries += 1
                    qa_text = "Acknowledging the user's response and asking a follow-up question to invite further reflection."
                    QUESTION_FOLLOWUP = await question_followup(last_bot_response, user_input, errors, tries, max_depth, conversation_context)    
                    "{:assistant} [@message QUESTION_FOLLOWUP_PRINT: print_text(QUESTION_FOLLOWUP)]"

                    log_messages(chat_logger, {"tries" : tries, "errors" : errors, "qa_text" : qa_text, "QUESTION_FOLLOWUP" :  QUESTION_FOLLOWUP}, level="info")
                    append_to_database(db,{
                        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "conversation_id": conversation_id,
                        "step_num": step_num,
                        "step_text": step_text,
                        "question_num": qq, 
                        "tries": tries,
                        "bot_output": QUESTION_FOLLOWUP,
                        "question_followup": True,
                        "type": "assistant_message"
                    })

                    continue_conversation = True
                else:
                    continue_conversation = False
            else:
                errors += 1
                qa_text = f"I hear what you're saying, but I want to make sure I fully understand your perspective on the question I asked. Would you mind elaborating a bit more? Attempt {errors}/{max_depth}."

                QUESTION_FOLLOWUP = await question_followup(question_text, user_input, errors, tries, max_depth, conversation_context)    
                "{:assistant} [@message QUESTION_FOLLOWUP_PRINT: print_text(QUESTION_FOLLOWUP)]"

                log_messages(chat_logger, {"tries" : tries, "errors" : errors, "qa_text" : qa_text, "QUESTION_FOLLOWUP" :  QUESTION_FOLLOWUP}, level="error")  
                append_to_database(db,{
                    "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "conversation_id": conversation_id,
                    "step_num": step_num,
                    "step_text": step_text,
                    "question_num": qq,
                    "errors": errors,
                    "bot_output": QUESTION_FOLLOWUP,
                    "question_followup": True,
                    "type": "assistant_message" 
                })
                
                if errors >= max_depth or tries >= max_depth:
                    continue_conversation = False  
                else:
                    continue_conversation = True
OUTRO = "Thank you so much for taking the time to share your experiences and perspectives with me today. I really enjoyed our conversation and feel like I've learned a lot. I appreciate your openness and willingness to engage on this important topic."
"{:assistant} [@message OUTRO_PRINT: print_text(OUTRO)]"