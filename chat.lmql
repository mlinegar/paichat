import lmql
from lmql.lib.chat import message
import json
import asyncio
import nest_asyncio
nest_asyncio.apply()
import pandas as pd
import logging

from utils import *

#  lmql.set_default_model('chatgpt')
lmql.set_default_model('gpt-4')
with open('immigration_script.json') as f:
    script = json.load(f)


    
chat_logger = setup_logger('chat_logger', 'test_chatbot.log')

#  for i, step in enumerate(script):
#      print(f"Step Number: {step['step_num']}")
#      print(f"Step Text: {step['step_text']}")
#      print(f"Step Questions: {step['questions']}")
    
system_prompt = "You are a trained volunteer canvasser named Brook.\
 Your goal is to have a conversation with the user to about workplace immigration raids by government agencies.\
 To do so, you use patience, empathy, and fact. You do not judge the user. You do not tell them they are wrong for holding their opinion.\
 You are not allowed to ask questions that are not in the script, other than asking the user to explain their position.\
 Remember that we are trying to be non-judgmental and to elicit narratives from the user about their experiences.\
 Remember that the user may be responding from their phone, so their responses may be short or incomplete.\
 As such we should accept any response that is relevant to the question situation, even if it may be offensive to some."

max_depth = 3

#  minimal working example (for debugging)
#  argmax(chunksize=128)
#  "{:system} {system_prompt}"
#  #  "{:assistant} {intro_text} [@message INTRO]"
#  while True:
#      user_input = await input()
#      "{:user} {user_input}"
#      "{:assistant} External Answer: [@message ANSWER]"

# FIXME: 
# may be worthwhile to overwrite the system prompt in each step
# this way can more explicitly guide the bot via programmatic script as well as system prompt
# e.g. directly read in the script description from the replication materials as part of the system prompt

argmax(chunksize=128)
"{:system} {system_prompt}"
#  "{:assistant} {intro_text} [@message INTRO]"
"{:assistant} Introduce yourself and then your topic to the user. Emphasize that you are here to learn about their experience. Do not ask the user any questions in this message.[@message INTRO]"
last_response = INTRO
# initialize empty data frame and overwrite old one
file_name = "qdf_test.csv"
qdf = pd.DataFrame()
qdf.to_csv(file_name)

for i, step in enumerate(script):
    # DEBUG
    if i < 2:
        continue
    step_num = step['step_num']
    step_check = f"Step Number: {step_num}"
    chat_logger.info(step_check)
    step_goal = step['step_goal']
    step_text = step['step_text']
    questions = step['questions']

    step_flag = None
    # Initialize user response as blank
    user_response = ""
    step_responses = "" # replace this with recallable info? or use as a supplement separately? add to ask_question?
    recalled_context = ""

    for qq, question in enumerate(questions):
        tries = 0
        errors = 0
        
        question_text = questions[qq]['question_text']
        if qq < len(questions) - 1:
            next_question_text = questions[qq+1]['question_text']
        else:
            next_question_text = "End of step: " + step_text

        requires_user_answer = question['requires_user_answer']
        allow_assistant_response = question['allow_assistant_response']
        flag_required = question['flag_required']
        generate_flag = question['generate_flag']
        verbatim = question['verbatim']
        recallable = question['recallable']
        max_length = question['max_length']
        use_tool = question['use_tool']
        tool_name = question['tool_name']
        log_messages(chat_logger, {"Step Number" : step_num, "Step Text" : step_text, "Question Number" : qq, "Question Text" : question_text, "Requires User Answer" : requires_user_answer, "Allow Assistant Response" : allow_assistant_response, "Flag Required" : flag_required, "Step Flag" : step_flag, "Generate Flag" : generate_flag, "Recallable" : recallable, "Verbatim" : verbatim})

        # when exactly do we want to skip a question?
        # if the flag is required, and the flag is not set or is false
        if (not flag_required is None) and (not step_flag == flag_required):
            continue

        QUESTION = await ask_question(question_text, last_response, user_response, verbatim, recalled_context=recalled_context, max_length=max_length, tool_name = tool_name)    
        "{:assistant} [@message QUESTION_PRINT: print_text(QUESTION)]"
        last_response = QUESTION
        log_messages(chat_logger, {"Question" :  QUESTION})

        continue_conversation = requires_user_answer
        while continue_conversation and tries <= max_depth and errors <= max_depth:
            user_input = await input()
            step_responses = step_responses + "\n" + user_input
            "{:user} {user_input}"    
            # Log user input
            log_messages(chat_logger, {"User Input": user_input}, level="info")
            if user_input == "SKIP":
                continue_conversation = False
                QUESTION_ANSWERED = False
                QUESTION_ANSWER = "SKIP"
                #  qdf = qdf.append({'step_num': step_num, 'step_text': step_text, 'question_num': qq, 'tries' : 0, 'question_text': question_text, 'bot_output' : QUESTION, 'user_input': user_input, 'answer' : QUESTION_ANSWER}, ignore_index=True)
                qdf = pd.concat([qdf, pd.DataFrame({'step_num': step_num, 'step_text': step_text, 'question_num': qq, 'tries' : 0, 'question_text': question_text, 'recallable' : recallable, 'bot_output' : QUESTION, 'user_input': user_input, 'answer' : QUESTION_ANSWER}, index=[0])], ignore_index=True)
                qdf.to_csv(file_name)
                #  qdf = append_and_save(qdf, file_name)
                continue
            
            # analyze user input
            QUESTION_ANSWERED, QUESTION_ANSWER, FOLLOW_UP, QA_REASON = await analyze_question_answer(question_text, user_input, step_responses)

            # Log QA results
            log_messages(chat_logger,
                            {"QUESTION_ANSWERED" : QUESTION_ANSWERED,
                            "QUESTION_ANSWER" : QUESTION_ANSWER,
                            "FOLLOW_UP" : FOLLOW_UP,
                            "QA_REASON" : QA_REASON})
            
            if QUESTION_ANSWERED:
                ### SAVE DATA ###
                #  qdf = qdf.append({'step_num': step_num, 'step_text': step_text, 'question_num': qq, 'tries' : tries, 'question_text': question_text, 'bot_output' : QUESTION, 'user_input': user_input, 'answer' : QUESTION_ANSWER}, ignore_index=True)
                qdf = pd.concat([qdf, pd.DataFrame({'step_num': step_num, 'step_text': step_text, 'question_num': qq, 'tries' : 0, 'question_text': question_text, 'recallable' : recallable, 'bot_output' : QUESTION, 'user_input': user_input, 'answer' : QUESTION_ANSWER}, index=[0])], ignore_index=True)
                qdf.to_csv(file_name)
                if recallable:
                    recalled_context = recalled_context + "\nQ:" + question_text + "\nA:" + QUESTION_ANSWER
                #  qdf = append_and_save(qdf, file_name)

                ### GENERATE FLAG ###
                if generate_flag:
                    flag_text = question['flag_text']
                    step_flag = await gen_flag(flag_text)
                    log_messages(chat_logger, {"Flag question" :  flag_text, "Step flag" :  step_flag})
                ### CHECK IF RESPONSE NEEDED ###
                if allow_assistant_response or FOLLOW_UP:
                    ASSISTANT_RESPONSE_NEEDED, USER_RESPONSE_NEEDED = await what_next("Current question: " + question_text + "\nNext question: " + next_question_text)
                    log_messages(chat_logger,
                        {"ASSISTANT_RESPONSE_NEEDED" : ASSISTANT_RESPONSE_NEEDED,
                        "USER_RESPONSE_NEEDED" : USER_RESPONSE_NEEDED})
                    # FIXME: add to being saved
                    if ASSISTANT_RESPONSE_NEEDED:
                        "{:assistant} Follow up message to the user: [@message QUESTION]"
                        log_messages(chat_logger, {"FOLLOW_UP_RESPONSE" : QUESTION})
                    if USER_RESPONSE_NEEDED:
                        tries += 1
                        continue_conversation = True
                    else:
                        continue_conversation = False
                else:
                    ASSISTANT_RESPONSE_NEEDED = False
                    USER_RESPONSE_NEEDED = False
                    continue_conversation = False
                log_messages(chat_logger,
                        {"ASSISTANT_RESPONSE_NEEDED" : ASSISTANT_RESPONSE_NEEDED,
                        "USER_RESPONSE_NEEDED" : USER_RESPONSE_NEEDED,
                        "FLAG_QUESTION" : question['flag_text'] if generate_flag else "N/A",
                        "STEP_FLAG" : step_flag if generate_flag else "N/A",
                        "continue_conversation" : continue_conversation})
            else:
                # If the question wasn't answered, log the reason and prepare for another attempt
                errors += 1
                qa_text = f'Error: I cannot accept this response as it does not answer the question. Please try again.\nAttempt {errors}/{max_depth}.\nError analysis:\n'
                "{:assistant} [@message ERROR: print_text(qa_text)]"
                "{:assistant} [@message ERROR: print_text(QA_REASON)]"
                log_messages(chat_logger, {"tries" : tries, "errors" : errors, "qa_text" : qa_text, "QA_REASON" :  QA_REASON}, level="error")
                continue_conversation = True
            