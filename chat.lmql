import lmql
from lmql.lib.chat import message
import json
import asyncio
import nest_asyncio
nest_asyncio.apply()
import logging
import pandas as pd
import logging
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
)  
from pathlib import Path
#  from tinydb import TinyDB, Query
#  from BetterJSONStorage import BetterJSONStorage
from tinydb import TinyDB
from tinydb.storages import JSONStorage
from tinydb.middlewares import CachingMiddleware
import uuid
#  from utils import *
# FIXME: allow option to accept user_id as input/read from url/file
#  user_id = "U12345678"
user_id = str(uuid.uuid4())
path = Path(f"output/{user_id}.db")
# set up database for storage
#  db = TinyDB(path, access_mode="r+", storage=BetterJSONStorage)
db = TinyDB(path, storage=CachingMiddleware(JSONStorage))
#  table = db.table(user_id)

def append_to_database(db, data):
    db.insert(data)
    db.storage.flush() # force save

#  lmql.set_default_model('chatgpt')
#  lmql.set_default_model('gpt-4')
with open('data/immigration_script.json') as f:
    full_script = json.load(f)
    script_details = full_script[0]
    script_description = script_details['script_description']
    script_length = len(full_script)
    script = full_script[1:script_length]


system_prompt = f"You are a trained volunteer canvasser named Brook.\
 Your goal is to have a conversation with the user about {script_description}.\
 To do so, you use patience, empathy, and fact. You do not judge the user. You do not tell them they are wrong for holding their opinion.\
 You are not allowed to ask questions that are not in the script, other than asking the user to explain their position.\
 Remember that we are trying to be non-judgmental and to elicit narratives from the user about their experiences.\
 Remember that the user may be responding from their phone, so their responses may be short or incomplete.\
 As such we should accept any response that is relevant to the question situation, even if it may be offensive to some."

max_depth = 3


def setup_logger(name, log_file, level=logging.INFO):
    """Function to setup as many loggers as you want"""
    handler = logging.FileHandler(log_file, mode='w')        
    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)

    return logger

chat_logger = setup_logger('chat_logger', 'logging/' + user_id + '.log')


def log_messages(chat_logger, messages, level="info"):
    """
    Logs messages with specified logging level in a structured manner.

    Args:
    - chat_logger: The logger object.
    - messages: A dictionary where keys are message labels and values are the messages themselves.
    - level: The logging level as a string (e.g., "info", "error").
    """
    for label, value in messages.items():
        formatted_message = f"{label}: {value}"
        if level == "info" : 
            chat_logger.info(formatted_message)
        elif level == "error" : 
            chat_logger.error(formatted_message)
        # Add more logging levels here as needed.

@lmql.query(chunksize=16)
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def yes_no():
    '''lmql
    "Respond only with one of: 'Yes.' or 'No.'."
    "A:[FLAG]" #where STOPS_AT(FLAG, '.') # FLAG in ['Yes.', 'No.'] and 
    return "yes" in FLAG.lower()
    '''

@lmql.query
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def chain_of_thought():
    '''lmql
    "What needs to happen?"
    "Think through the question step by step.[THOUGHTS]"
    return THOUGHTS
    '''
        
# takes in and returns text
# used with @message to print text to the user without needing to invoke LLM
@lmql.query
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def print_text(text):
    '''lmql
    "{text}"
    '''

@lmql.query
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def include(text):
    '''lmql
    #  "Your response must include the following text verbatim: {text}"
    "Your response should rephrase the following as a friendly question to the user: {text}"
    "[RESPONSE]"
    return RESPONSE
    '''

# FIXME: this is clumsy, and requires each tool individually to be added
@lmql.query(model=lmql.model('openai/gpt-4'))
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def use_tool(tool_name):
    '''lmql
    if tool_name == "immigration_story":
        "Recall a story from your training data that centers on the lived experience of an 'average' immigrant to the United States (for example: a refugee/asylum seeker, a farm worker, an undocumented parent)."
        "Our goal is to utilize perspective-sharing to reduce prejudice with regard to immigration, specifically, to reduce people's stated preference for large-scale arrests and detainment of immigrants at their place of work."
        "We want to recreate stories about immigration that volunteer canvassers might authentically share, stories that are personal, heartfelt, and real, designed to elicit sympathy and empathy for immigrants who would otherwise be arrested in mass detentions."
        "What types of personal stories will inspire empathy in the user?[COT: chain_of_thought]"
        "Now that you've thought about it, tell your story to the user."
    else:
        "ERROR: Tool not found."
    "[RESPONSE]"
    return RESPONSE
    '''

@lmql.query(model=lmql.model('openai/gpt-4'))
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def ask_question(question_text, last_bot_response, user_response, verbatim, recalled_context="", max_length=256, tool_name=""):
    '''lmql
    "Reminder: the last thing you, the assistant, said was: {last_bot_response}"
    if not user_response=="":
        "Reminder: the user said {user_response}."
    if not recalled_context=="":
        "Reminder: some information about the user: {recalled_context}."
    "Make sure to phrase what you say so that it feels like a natural response to what the user said."
    "If the user asked any questions, make sure to answer them."
    "If the user addressed any points, make sure to address them."
    "Make sure your entire response will make sense to the user."
    if verbatim and tool_name =="":
        "Ask the user:[QUESTION: include(question_text)]"
    elif (not verbatim) and (not tool_name == ""):
        "[QUESTION: use_tool(tool_name)]"
    #  elif (not verbatim) and (tool_name == ""):
        #  "ERROR: Tool use requested but which tool was not specified. Explain the error.[QUESTION]"
    elif not verbatim:
        "Rephrase the following as a question to the user:{question_text}[QUESTION]" where len(TOKENS(QUESTION)) < max_length
    else: 
        # this should never happen
        "Rephrase the following as a question to the user:{question_text}[QUESTION]" where len(TOKENS(QUESTION)) < max_length
    return QUESTION
    '''

@lmql.query(model=lmql.model('openai/gpt-3.5-turbo'))
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def did_user_answer(question_text, answer_text, prev_answer_text):
    '''lmql
    "Question: {question_text}"
    "User answer: {answer_text}"
    "Previous user answer: {prev_answer_text}"
    "Reminder: the user may be responding from their phone, so their responses may be short or incomplete. You should accept most user responses that are even vaguely relevant to the question."
    "We need to answer the following question:"
    "Has the user answered the question, either in this response or a previous one?"
    "[COT: chain_of_thought]"
    "Has the user answered the question?[ANSWERED: yes_no]"
    return ANSWERED
    '''

@lmql.query(model=lmql.model('openai/gpt-3.5-turbo'))
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def get_user_answer(question_text, answer_text, prev_answer_text):
    '''lmql
    "Question: {question_text}"
    "User answer: {answer_text}"
    "Previous user answer: {prev_answer_text}"
    "Reminder: the user may be responding from their phone, so their responses may be short or incomplete. You should accept most user responses that are even vaguely relevant to the question."
    "We need to answer the following question:"
    "What was the user's answer to the question?"
    "[COT: chain_of_thought]"
    "If the user did not answer the question, simply respond with 'None'."
    "What was the user's answer?[ANSWER]"
    return ANSWER
    '''

@lmql.query(model=lmql.model('openai/gpt-3.5-turbo'))
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def assistant_response_needed(question_text, answer_text, prev_answer_text):
    '''lmql
    "Question: {question_text}"
    "User answer: {answer_text}"
    "Reminder: the user may be responding from their phone, so their responses may be short or incomplete. You should accept most user responses that are even vaguely relevant to the question."
    "We need to answer the following question:"
    "Is a response from the assistant needed before moving on to the next question?"
    "This will happen when the user's response is unclear, incomplete, or when the user asks for more information."
    "Unless you have strong evidence to the contrary, assume that the answer is 'no'."
    "[COT: chain_of_thought]"
    "Is a response from the assistant needed before moving on to the next question?[ASSISTANT_RESPONSE_NEEDED: yes_no]"
    return ASSISTANT_RESPONSE_NEEDED
    '''

# FIXME: incorporate what next into this async call
# then everything will be in parallel
# replace with logic for whether need bot response before asking next question
# e.g. if user hasn't answered or if they asked for more info
# capture ASSISTANT_RESPONSE_NEEDED here
# maybe: in follow up, check if user may want to respond to bot before moving on to next question
# then what_next not needed!
# logic of ordering for wrapper function:
# if question not answered or if bot response needed, always feed to follow up (so these should take precedence)
# if user answered...
# FIXME: add logic: if question was answered, record it and add this info to follow up (e.g. we want to move on)

# wrapper function for multiple async calls
#  @lmql.query(model=lmql.model('openai/gpt-3.5-turbo'))
#  @retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
#  async def analyze_question_answer(question_text, answer_text, prev_answer_text):
#  @retry(wait=wait_random_exponential(min=1, max=30), stop=stop_after_attempt(6))
#  async def execute_response_analysis(question_text, answer_text, prev_answer_text):
#      user_answered_task = asyncio.create_task(did_user_answer(question_text, answer_text, prev_answer_text))
#      user_answer_task = asyncio.create_task(get_user_answer(question_text, answer_text, prev_answer_text))
#      assistant_response_needed_task = asyncio.create_task(assistant_response_needed(question_text, answer_text, prev_answer_text))
    
#      while True:
#          done, _ = await asyncio.wait([user_answered_task, user_answer_task, assistant_response_needed_task], return_when=asyncio.FIRST_COMPLETED)
        
#          if user_answered_task in done:
#              user_answered = user_answered_task.result()
            
#              if not user_answered:
#                  return False, "", True
            
#              elif user_answer_task in done and assistant_response_needed_task in done:
#                  user_answer = user_answer_task.result()
#                  assistant_response_needed = assistant_response_needed_task.result()
                
#                  return True, user_answer, assistant_response_needed
        
#          else:
#              await asyncio.sleep(0.1)

@retry(wait=wait_random_exponential(min=1, max=30), stop=stop_after_attempt(6))
async def execute_response_analysis(question_text, answer_text, prev_answer_text):
    user_answered_task = asyncio.create_task(did_user_answer(question_text, answer_text, prev_answer_text))
    user_answer_task = asyncio.create_task(get_user_answer(question_text, answer_text, prev_answer_text))
    assistant_response_needed_task = asyncio.create_task(assistant_response_needed(question_text, answer_text, prev_answer_text))

    user_answered_ANS = await user_answered_task

    if not user_answered_ANS:
        return False, "", True

    # Await other tasks only if necessary
    user_answer_ANS = await user_answer_task
    assistant_response_needed_ANS = await assistant_response_needed_task

    return True, user_answer_ANS, assistant_response_needed_ANS

# FIXME: replace this with multiple async calls
# like here: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_use_guardrails.ipynb
# (section 7)
# takes waaaaaay too long to run and too many tokens
#  @lmql.query(model=lmql.model('openai/gpt-3.5-turbo'))
#  @retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
#  async def what_next(question_context):
#      '''lmql
#      "Should we continue the current conversation or move on to the next question?"
#      "Context: {question_context}"
#      "We need to move through this survey quickly, but we also want the user to share their experiences and to show that we hear them."
#      #  "Does the assistant need to respond to the user's last message rather than ask the next question?"
#      #  "[COT: chain_of_thought]"
#      #  "[ASSISTANT_RESPONSE_NEEDED: yes_no]"
#      ASSISTANT_RESPONSE_NEEDED = True
#      if ASSISTANT_RESPONSE_NEEDED:
#          "Does the user seem engaged and wanting to continue this exact specific part of the conversation? (Assume no)"
#          "[COT2: chain_of_thought]"
#          "[USER_RESPONSE_NEEDED: yes_no]"
#      else:
#          USER_RESPONSE_NEEDED = False
#      return ASSISTANT_RESPONSE_NEEDED, USER_RESPONSE_NEEDED
    #  '''
    
@lmql.query(model=lmql.model('openai/gpt-3.5-turbo'))
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def gen_flag(flag_text):
    '''lmql
    "{flag_text}"
    "[COT: chain_of_thought]"
    "[ANS: yes_no]"
    return ANS
    '''

@lmql.query(model=lmql.model('openai/gpt-4'))
@retry(wait=wait_random_exponential(min=0.1, max=6), stop=stop_after_attempt(6))
async def question_followup(question, user_input, errors, tries, max_depth):
    '''lmql
    "The user did not respond to the question."
    "The question that needs to be answered is: {question}"
    "The user's response was: {user_input}"
    "Write a friendly and curious follow-up response to the user, responding to what they said and then asking the question again."
    "Be sure to respond to what the user said."
    "If they asked for more information, make sure to provide it."
    if errors >= max_depth or tries >= max_depth:
        "Note: you have tried several times to get the user to answer the question."
        "We are now moving on to the next question."
        "Make sure your response feels natural to the user in moving on to the next question."
        "Tell the user that, in the interest of time, we are moving on to the next question."
    "[FOLLOW_UP]"
    return FOLLOW_UP
    '''

def append_and_save(df, file_name):
    df = pd.concat([df, pd.DataFrame({'step_num': step_num, 'step_text': step_text, 'question_num': qq, 'tries' : 0, 'question_text': question_text, 'bot_output' : QUESTION, 'user_input': user_input, 'answer' : QUESTION_ANSWER}, index=[0])], ignore_index=True)
    df.to_csv(file_name)
    return df    

#  minimal working example (for debugging)
#  argmax(chunksize=128)
#  "{:system} {system_prompt}"
#  #  "{:assistant} {intro_text} [@message INTRO]"
#  while True:
#      user_input = await input()
#      "{:user} {user_input}"
#      "{:assistant} External Answer: [@message ANSWER]"

# FIXME: 
# may be worthwhile to overwrite the system prompt in each step
# this way can more explicitly guide the bot via programmatic script as well as system prompt
# e.g. directly read in the script description from the replication materials as part of the system prompt

argmax(chunksize=128)
"{:system} {system_prompt}"
#  "{:assistant} {intro_text} [@message INTRO]"
"{:assistant} Introduce yourself and then your topic to the user. Emphasize that you are here to learn about their experience. Do not ask the user any questions in this message.[@message INTRO]"
last_bot_response = INTRO

for i, step in enumerate(script):
    
    step_num = step['step_num']
    step_check = f"Step Number: {step_num}"
    chat_logger.info(step_check)
    step_goal = step['step_goal']
    step_text = step['step_text']
    questions = step['questions']

    step_flag = None
    # Initialize user response as blank
    user_response = ""
    step_responses = "" # replace this with recallable info? or use as a supplement separately? add to ask_question?
    recalled_context = ""

    for qq, question in enumerate(questions):
        tries = 0
        errors = 0
        
        question_text = questions[qq]['question_text']
        if qq < len(questions) - 1:
            next_question_text = questions[qq+1]['question_text']
        else:
            next_question_text = "End of step: " + step_text

        requires_user_answer = question['requires_user_answer']
        allow_assistant_response = question['allow_assistant_response']
        flag_required = question['flag_required']
        generate_flag = question['generate_flag']
        verbatim = question['verbatim']
        recallable = question['recallable']
        max_length = question['max_length']
        use_tool = question['use_tool']
        tool_name = question['tool_name']
        log_messages(chat_logger, {"Step Number" : step_num, "Step Text" : step_text, "Question Number" : qq, "Question Text" : question_text, "Requires User Answer" : requires_user_answer, "Allow Assistant Response" : allow_assistant_response, "Flag Required" : flag_required, "Step Flag" : step_flag, "Generate Flag" : generate_flag, "Recallable" : recallable, "Verbatim" : verbatim})
        
        # when exactly do we want to skip a question?
        # if the flag is required, and the flag is not set or is false
        if (not flag_required is None) and (not step_flag == flag_required):
            continue

        QUESTION = await ask_question(question_text, last_bot_response, user_response, verbatim, recalled_context=recalled_context, max_length=max_length, tool_name = tool_name)    
        "{:assistant} [@message QUESTION_PRINT: print_text(QUESTION)]"

        last_bot_response = QUESTION
        log_messages(chat_logger, {"Question" :  QUESTION})

        append_to_database(db,{
            "step_num": step_num,
            "step_text": step_text,
            "question_num": qq,
            "question_text": question_text,
            "bot_output": QUESTION
        })

        continue_conversation = requires_user_answer
        while continue_conversation and tries <= max_depth and errors <= max_depth:
            
            user_input = await input()
            step_responses = step_responses + "\n" + user_input
            "{:user} {user_input}"    

            # Log user input
            log_messages(chat_logger, {"User Input": user_input}, level="info")

            append_to_database(db,{
                "step_num": step_num,
                "step_text": step_text,
                "question_num": qq,
                "tries": tries,
                "question_text": question_text,
                "recallable": recallable,
                "user_input": user_input
            })

            if user_input == "SKIP":
                continue_conversation = False
                QUESTION_ANSWERED = False
                QUESTION_ANSWER = "SKIP"
                continue
            # Analyze user input using the asynchronous function
            QUESTION_ANSWERED, QUESTION_ANSWER, ASSISTANT_RESPONSE_NEEDED = await execute_response_analysis(question_text, user_input, step_responses)

            # Log QA results
            log_messages(chat_logger,
                            {"QUESTION_ANSWERED" : QUESTION_ANSWERED,
                            "QUESTION_ANSWER" : QUESTION_ANSWER,
                            "ASSISTANT_RESPONSE_NEEDED" : ASSISTANT_RESPONSE_NEEDED})

            append_to_database(db,{
                "step_num": step_num,
                "step_text": step_text,
                "question_num": qq,
                "tries": tries,
                "question_text": question_text,
                "recallable": recallable,
                "bot_output": QUESTION,
                "user_input": user_input,
                "answer": QUESTION_ANSWER,
                "answered": QUESTION_ANSWERED,
                "assistant_response_needed": ASSISTANT_RESPONSE_NEEDED
            })

            if QUESTION_ANSWERED:
                ### SAVE DATA ###
                if recallable:
                    recalled_context = recalled_context + "\nQ:" + question_text + "\nA:" + QUESTION_ANSWER

                ### GENERATE FLAG ###
                if generate_flag:
                    flag_text = question['flag_text']
                    step_flag = await gen_flag(flag_text)
                    log_messages(chat_logger, {"Flag question" :  flag_text, "Step flag" :  step_flag})
                    append_to_database(db,{
                        "step_num": step_num,
                        "step_text": step_text,
                        "question_num": qq,
                        "tries": tries,
                        "flag_text": flag_text,
                        "step_flag": step_flag
                    })
                
                if ASSISTANT_RESPONSE_NEEDED:
                    tries += 1
                    qa_text = "Following up to the user's response."
                    QUESTION_FOLLOWUP = await question_followup(last_bot_response, user_input, errors, tries, max_depth)    
                    "{:assistant} [@message QUESTION_FOLLOWUP_PRINT: print_text(QUESTION_FOLLOWUP)]"

                    log_messages(chat_logger, {"tries" : tries, "errors" : errors, "qa_text" : qa_text, "QUESTION_FOLLOWUP" :  QUESTION_FOLLOWUP}, level="info")
                    append_to_database(db,{
                        "step_num": step_num,
                        "step_text": step_text,
                        "question_num": qq,
                        "tries": tries,
                        "bot_output": QUESTION_FOLLOWUP,
                        "question_followup": True
                    })

                    continue_conversation = True
                else:
                    continue_conversation = False
            else:
                # If the question wasn't answered, log the reason and prepare for another attempt
                errors += 1
                qa_text = f'Error: I cannot accept this response as it does not answer the question. Please try again.\nAttempt {errors}/{max_depth}.\nError analysis:\n'

                QUESTION_FOLLOWUP = await question_followup(question_text, user_input, errors, tries, max_depth)    
                "{:assistant} [@message QUESTION_FOLLOWUP_PRINT: print_text(QUESTION_FOLLOWUP)]"

                log_messages(chat_logger, {"tries" : tries, "errors" : errors, "qa_text" : qa_text, "QUESTION_FOLLOWUP" :  QUESTION_FOLLOWUP}, level="error")
                append_to_database(db,{
                    "step_num": step_num,
                    "step_text": step_text,
                    "question_num": qq,
                    "errors": errors,
                    "bot_output": QUESTION_FOLLOWUP,
                    "question_followup": True
                })
                
                if errors >= max_depth or tries >= max_depth:
                    continue_conversation = False
                else:
                    continue_conversation = True